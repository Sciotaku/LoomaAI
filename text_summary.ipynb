{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch\n",
    "! pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from io import StringIO\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, start_page, end_page):\n",
    "    # Extracts text from a PDF file, specifying the pages for Chapter 1\n",
    "    text = \"\"\n",
    "    output = StringIO()\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        extract_text_to_fp(f, output, page_numbers=range(start_page-1, end_page))\n",
    "        text = output.getvalue()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF and the pages to extract\n",
    "pdf_path = \"looma_sample_book.pdf\"  # Update this to the path of your PDF file\n",
    "start_page = 3  # Update this to the start page of Chapter 1\n",
    "end_page = 4  # Update this to the end page of Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the specified pages\n",
    "chapter_text = extract_text_from_pdf(pdf_path, start_page, end_page)\n",
    "print(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Summarize Text Using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_text(text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    summary_text = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "    return summary_text[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"looma_sample_book.pdf\"  # Update this to the path of your PDF file\n",
    "start_page = 1  # Update this to the start page of Chapter 1\n",
    "end_page = 2  # Update this to the end page of Chapter 1\n",
    "\n",
    "# Extract text\n",
    "chapter_text = extract_text_from_pdf(pdf_path, start_page, end_page)\n",
    "\n",
    "# Generate summary\n",
    "chapter_summary = summarize_text(chapter_text)\n",
    "print(chapter_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples['article']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples['article']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['highlights'], max_length=150, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from io import StringIO\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, start_page, end_page):\n",
    "    text = \"\"\n",
    "    output = StringIO()\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        extract_text_to_fp(f, output, page_numbers=range(start_page-1, end_page))\n",
    "        text = output.getvalue()\n",
    "    return text\n",
    "\n",
    "# Define the path to the PDF and the pages to extract\n",
    "pdf_path = \"looma_sample_book.pdf\"  # Update this to the path of your PDF file\n",
    "start_page = 1  # Update this to the start page of Chapter 1\n",
    "end_page = 2  # Update this to the end page of Chapter 1\n",
    "\n",
    "# Extract text from the specified pages\n",
    "chapter_text = extract_text_from_pdf(pdf_path, start_page, end_page)\n",
    "print(\"Extracted Chapter Text:\")\n",
    "print(chapter_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./results/t5_fine_tuned\"  # Path to the fine-tuned model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Function to generate a summary\n",
    "def generate_summary(chapter_text):\n",
    "    inputs = tokenizer(\"summarize: \" + chapter_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate and print the summary for the extracted chapter text\n",
    "summary = generate_summary(chapter_text)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymupdf pytesseract pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Set the path to the Tesseract executable if it's not in your PATH\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/tesseract'  # Update this path if necessary\n",
    "\n",
    "def extract_text_and_images_from_pdf(pdf_path, image_output_folder):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    extracted_texts = []\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document[page_num]\n",
    "        text = page.get_text()\n",
    "        extracted_texts.append({'page': page_num, 'text': text})\n",
    "        \n",
    "        # Extract images\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(image_output_folder, f\"page_{page_num}_img_{img_index}.{image_ext}\")\n",
    "            image.save(image_path, format=image_ext.upper())\n",
    "    \n",
    "    return extracted_texts\n",
    "\n",
    "# Example usage\n",
    "pdf_path = 'path_to_your_pdf_file.pdf'\n",
    "image_output_folder = 'path_to_output_images_folder'\n",
    "\n",
    "if not os.path.exists(image_output_folder):\n",
    "    os.makedirs(image_output_folder)\n",
    "\n",
    "extracted_texts = extract_text_and_images_from_pdf(pdf_path, image_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply OCR to all images in the folder\n",
    "ocr_texts = []\n",
    "\n",
    "for filename in os.listdir(image_output_folder):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "        image_path = os.path.join(image_output_folder, filename)\n",
    "        text = ocr_image(image_path)\n",
    "        if text:\n",
    "            ocr_texts.append({'image_filename': filename, 'text': text})\n",
    "\n",
    "# Convert the OCR texts to a DataFrame\n",
    "ocr_df = pd.DataFrame(ocr_texts)\n",
    "print(ocr_df.head())\n",
    "\n",
    "# Save the OCR texts to a CSV file\n",
    "ocr_df.to_csv('ocr_extracted_texts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OCR extracted texts\n",
    "ocr_df = pd.read_csv('ocr_extracted_texts.csv')\n",
    "\n",
    "# Combine extracted texts and OCR texts\n",
    "combined_texts = []\n",
    "\n",
    "for text in extracted_texts:\n",
    "    page_num = text['page']\n",
    "    text_content = text['text']\n",
    "    \n",
    "    # Add OCR text if available\n",
    "    ocr_text_content = \"\"\n",
    "    matching_ocr = ocr_df[ocr_df['image_filename'].str.contains(f\"page_{page_num}_img_\")]\n",
    "    if not matching_ocr.empty:\n",
    "        ocr_text_content = \" \".join(matching_ocr['text'].tolist())\n",
    "    \n",
    "    combined_text = text_content + \" \" + ocr_text_content\n",
    "    combined_texts.append({'page': page_num, 'text': combined_text})\n",
    "\n",
    "combined_df = pd.DataFrame(combined_texts)\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save the combined texts to a CSV file\n",
    "combined_df.to_csv('combined_texts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the combined text\n",
    "combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['page', 'cleaned_text']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['page', 'cleaned_text']])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a CSV file with columns 'chapter' and 'text'\n",
    "df = pd.read_csv('grade10_science_textbook.csv')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/llama-7b')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['text']\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('meta-llama/llama-7b')\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation using ROUGE\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {key: value.mid.fmeasure for key, value in result.items()}\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example usage\n",
    "text = \"Your textbook chapter text here.\"\n",
    "summary = summarizer(text, max_length=150, min_length=40, do_sample=False)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/summarize', methods=['POST'])\n",
    "def summarize():\n",
    "    content = request.json\n",
    "    text = content['text']\n",
    "    summary = summarizer(text, max_length=150, min_length=40, do_sample=False)\n",
    "    return jsonify(summary)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
